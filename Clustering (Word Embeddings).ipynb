{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim\n",
    "import os\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk.tokenize.punkt\n",
    "from gensim.utils import lemmatize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "#import Pattern\n",
    "from textblob import TextBlob,Word\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.test.utils import datapath, get_tmpfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitter(dataset):\n",
    "    #dataset=data.copy(deep=True)\n",
    "    dataset['SplitText']=''\n",
    "    index=0\n",
    "    for query_raw in dataset['Problem Statement']:\n",
    "        separator_found = False\n",
    "        if (type(query_raw) != str): #check for nan values\n",
    "            #print(query)\n",
    "            continue\n",
    "        query=re.sub('[^\\w\\d\\s]+',' ',query_raw)\n",
    "        query=re.sub('\\s+',' ',query)\n",
    "        #print(query)\n",
    "        words_in_query = query.split()\n",
    "        for word_index in range(1, len(words_in_query)):\n",
    "            stemmed_lower_word = snowball.stem(words_in_query[word_index].lower())\n",
    "            #print(\"{}-------------{}\".format(words_in_query[word_index], stemmed_lower_word))\n",
    "            if (stemmed_lower_word in distinct_stemmed_separators):\n",
    "                #print('YES')\n",
    "                #found the separator word, now seperate the raw query\n",
    "                #general_info.append(' '.join(words_in_query[0:word_index]))\n",
    "                text=' '.join(words_in_query[word_index:len(words_in_query)])\n",
    "                dataset.at[index,'SplitText']=text\n",
    "                #dataset.at[i,'SplitText']='asdfg'\n",
    "                #print(dataset.loc[index,'SplitText'])\n",
    "                separator_found = True\n",
    "                index+=1\n",
    "                break\n",
    "        if (separator_found == False):\n",
    "            #false_count+=1\n",
    "            #if there is totally no separator word in the query, add in the whole query\n",
    "            #separator_found_false_count += 1\n",
    "            #print(query)\n",
    "            #print(\"------------------------------------------------------------------------------------------\\n\\n\")\n",
    "            #general_info.append(\" \")\n",
    "            #actual_query.append(query)\n",
    "            dataset.at[index,'SplitText']=query\n",
    "            index+=1\n",
    "    return dataset\n",
    "\n",
    "\n",
    "## Need to install averaged_perceptron_tagger in a folder named 'taggers' first\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.VERB)\n",
    "\n",
    "NON_ALPHANUM=re.compile(r'[\\W]')\n",
    "NON_ASCII=re.compile(r'[^a-z0-1\\s]')\n",
    "\n",
    "## CREATE LEMMATIZER OBJECT\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "def normalize_text(texts):\n",
    "    lower=texts.lower()\n",
    "    no_punctuation=NON_ALPHANUM.sub(r' ',lower)\n",
    "    no_non_ascii=NON_ASCII.sub(r'',no_punctuation)\n",
    "    return no_non_ascii\n",
    "\n",
    "def clean_text(dataset):\n",
    "    dataset['Problem Statement Filtered']=dataset['SplitText'].apply(normalize_text)\n",
    "    dataset['Problem Statement Filtered']=dataset['Problem Statement Filtered'].apply(lambda x: ' '.join([lemmatizer.lemmatize(w,get_wordnet_pos(w)) for w in x.split() if len(w)>2\n",
    "                                                                             and not lemmatizer.lemmatize(w,get_wordnet_pos(w)) in all_stopwords]))\n",
    "    return dataset\n",
    "                        \n",
    "\n",
    "def get_top_n_words(corpus, n=10):\n",
    "  vec = CountVectorizer(stop_words=all_stopwords,ngram_range=(1,2)).fit(corpus)\n",
    "  bag_of_words = vec.transform(corpus)\n",
    "  sum_words = bag_of_words.sum(axis=0) \n",
    "  words_freq = [(word, sum_words[0, idx]) for word, idx in   vec.vocabulary_.items()]\n",
    "  words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "  return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.data.path.append(\"C:/Users/ESG-JT5190/Documents/Python Scripts\")\n",
    "os.getcwd()\n",
    "os.chdir(r'C:\\Users\\ESG-JT5190\\Documents\\Project Code Repo\\data')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('./processed/data_consolidated_20200629.csv',engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[1,'Problem Statement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data20=data[data.year==20]\n",
    "print('Number of rows and columns for 2020 dataset is {}'.format(data20.shape))\n",
    "\n",
    "data19=data[data.year==19]\n",
    "print('Number of rows and columns for 2019 dataset is {}'.format(data19.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data20.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Others20=data20[(data20.category_digital==False) & (data20.category_manpower==False)]\n",
    "print('Number of rows and columns for 2020 Others dataset is {}'.format(Others20.shape))\n",
    "Digital20=data20[data20.category_digital==True]\n",
    "print('Number of rows and columns for 2020 Digital dataset is {}'.format(Digital20.shape))\n",
    "Manpower20=data20[data20.category_manpower==True]\n",
    "print('Number of rows and columns for 2020 Manpower dataset is {}'.format(Manpower20.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use 2 months for Jan and Feb\n",
    "Others20=Others20[(Others20['month']=='Jan') | (Others20['month']=='Feb')]\n",
    "print('Number of rows and columns for 2020 Others (Jan and Feb) dataset is {}'.format(Others20.shape))\n",
    "Digital20=Digital20[(Digital20['month']=='Jan') | (Digital20['month']=='Feb')]\n",
    "print('Number of rows and columns for 2020 Digital (Jan and Feb) dataset is {}'.format(Digital20.shape))\n",
    "Manpower20=Manpower20[(Manpower20['month']=='Jan') | (Manpower20['month']=='Feb')]\n",
    "print('Number of rows and columns for 2020 Manpower (Jan and Feb) dataset is {}'.format(Manpower20.shape))\n",
    "\n",
    "Others20=Others20.reset_index(drop=True)\n",
    "Digital20=Digital20.reset_index(drop=True)\n",
    "Manpower20=Manpower20.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USE LIST OF STOPWORDS FROM R LIBRARY COMBINED WITH CUSTOM STOPWORDS AS I FELT THAT THE STOPWORDS PROVIDED BY PYTHON\n",
    "### LIBRARIES ARE NOT ENOUGH. WORDS SUCH AS 'LIKE' ARE NOT STOPWORDS IN PYTHON LIBRARIES\n",
    "all_stopwords=open('./raw/all_stopwords.txt','r')\n",
    "all_stopwords=all_stopwords.read().split('\\n')\n",
    "len(all_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball = SnowballStemmer(\"english\")\n",
    "\n",
    "## READ IN SEPARATOR WORDS\n",
    "raw_separators = open(r'.\\raw\\separators.txt','r')\n",
    "raw_separators=raw_separators.read().split('\\n')\n",
    "#additional_stopwords\n",
    "print(\"Length: {}\\nList: {}\\n\".format(len(raw_separators), sorted(raw_separators)))\n",
    "\n",
    "\n",
    "stemmed_separators = []\n",
    "\n",
    "for separator in raw_separators:\n",
    "    stemmed_separators.append(snowball.stem(separator))\n",
    "\n",
    "print(\"Length: {}\\nList: {}\\n\".format(len(stemmed_separators), sorted(stemmed_separators)))\n",
    "\n",
    "distinct_stemmed_separators = sorted(list(set(stemmed_separators)))\n",
    "\n",
    "print(\"Length: {}\\nList: {}\\n\".format(len(distinct_stemmed_separators), distinct_stemmed_separators))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DO OTHERS SUBSET FIRST\n",
    "## SPLIT THE TEXT FOR OTHERS SUBSET\n",
    "Others20=splitter(Others20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Others20['SplitText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CLEAN THE TEXT\n",
    "Others20=clean_text(Others20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Others20.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direc=os.path.dirname(os.getcwd())\n",
    "print(direc)\n",
    "glove_filename=os.path.join(direc,'data','raw','glove.6B.100d.txt')\n",
    "print(glove_filename)\n",
    "direc=os.path.dirname(os.getcwd())\n",
    "tmp_filename=os.path.join(direc,'data','raw','tmp_file.txt')\n",
    "print(tmp_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### READ IN WORD EMBEDDINGS FILE\n",
    "glove_file=datapath(glove_filename)\n",
    "tmp_file=get_tmpfile(tmp_filename)\n",
    "\n",
    "_=glove2word2vec(glove_file,tmp_file)\n",
    "\n",
    "word2vec=KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word='iraq'\n",
    "print('Word: {}'.format(word))\n",
    "print('First 20 values of embedding:\\n{}'.format(word2vec[word][:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word2vec.most_similar(positive=['woman', 'king'], negative=['man'], topn=3))\n",
    "print(word2vec.most_similar(positive=['tennis', 'ronaldo'], negative=['soccer'], topn=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordVecVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = 100   \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        return self    \n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in texts.split() if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for texts in X\n",
    "        ])#representing each statement by the mean of word embeddings for the words used in the statement.\n",
    "\n",
    "\n",
    "wtv_vect = WordVecVectorizer(word2vec)\n",
    "X_train_wtv_others = wtv_vect.transform(Others20['Problem Statement Filtered'])\n",
    "print(X_train_wtv_others.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_number=5\n",
    "\n",
    "km = KMeans(\n",
    "    n_clusters=clusters_number, init='random',\n",
    "    n_init=10, max_iter=300, \n",
    "    tol=1e-04, random_state=1000\n",
    ")\n",
    "y_km_others = km.fit_predict(X_train_wtv_others)\n",
    "clusters_df_others = pd.DataFrame({'Problem Statement Filtered' :Others20['Problem Statement Filtered'], 'topic_cluster' :y_km })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_df_others[clusters_df_others['topic_cluster']==4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,clusters_number):\n",
    "    globals()['Cluster_%s_Others' %i]=clusters_df_others[clusters_df_others['topic_cluster']==i]\n",
    "    print('Cluster',i,'has dimensions',globals()['Cluster_%s_Others' %i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(0,clusters_number):\n",
    "    #print(type(i))\n",
    "    words = []\n",
    "    word_values = []\n",
    "    for i,j in get_top_n_words(globals()['Cluster_%s_Others' %k]['Problem Statement Filtered'],10):\n",
    "      words.append(i)\n",
    "      word_values.append(j)\n",
    "    print('Top 10 words for CLuster',k,':',\", \".join(words),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DO DIGITAL NOW\n",
    "## SPLIT THE TEXT FOR Digital SUBSET\n",
    "Digital20=splitter(Digital20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Digital20['SplitText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CLEAN THE TEXT\n",
    "Digital20=clean_text(Digital20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_wtv_digi = wtv_vect.transform(Digital20['Problem Statement Filtered'])\n",
    "print(X_train_wtv_digi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_km_digi = km.fit_predict(X_train_wtv_digi)\n",
    "clusters_df_digi = pd.DataFrame({'Problem Statement Filtered' :Digital20['Problem Statement Filtered'], 'topic_cluster' :y_km_digi })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,clusters_number):\n",
    "    globals()['Cluster_%s_digi' %i]=clusters_df_digi[clusters_df_digi['topic_cluster']==i]\n",
    "    print('Cluster',i,'has dimensions',globals()['Cluster_%s_digi' %i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(0,clusters_number):\n",
    "    #print(type(i))\n",
    "    words = []\n",
    "    word_values = []\n",
    "    for i,j in get_top_n_words(globals()['Cluster_%s_digi' %k]['Problem Statement Filtered'],10):\n",
    "      words.append(i)\n",
    "      word_values.append(j)\n",
    "    print('Top 10 words for Cluster',k,':',\", \".join(words),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DO MANPOWER NOW\n",
    "## SPLIT THE TEXT FOR Digital SUBSET\n",
    "Manpower20=splitter(Manpower20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CLEAN THE TEXT\n",
    "Manpower20=clean_text(Manpower20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_wtv_man = wtv_vect.transform(Manpower20['Problem Statement Filtered'])\n",
    "print(X_train_wtv_man.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_km_man = km.fit_predict(X_train_wtv_man)\n",
    "clusters_df_man = pd.DataFrame({'Problem Statement':Manpower20['Problem Statement']\n",
    "    ,'SplitText':Manpower20['SplitText'],\n",
    "    'Problem Statement Filtered' :Manpower20['Problem Statement Filtered'], 'topic_cluster' :y_km_man })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,clusters_number):\n",
    "    globals()['Cluster_%s_man' %i]=clusters_df_man[clusters_df_man['topic_cluster']==i]\n",
    "    print('Cluster',i,'has dimensions',globals()['Cluster_%s_man' %i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(0,clusters_number):\n",
    "    #print(type(i))\n",
    "    words = []\n",
    "    word_values = []\n",
    "    for i,j in get_top_n_words(globals()['Cluster_%s_man' %k]['Problem Statement Filtered'],10):\n",
    "      words.append(i)\n",
    "      word_values.append(j)\n",
    "    print('Top 10 words for Cluster',k,':',\", \".join(words),'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
